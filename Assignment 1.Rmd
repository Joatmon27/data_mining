---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

## Question 1
For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) The sample size $n$ is extremely large, and the number of predictors
$p$ is small.

*Depends on what you are aiming for, if interpretability is super important with the few predictors, then generally a more inflexible method will be sufficient.  However, as machine learning requires very large data sets, it will do well, especially when the $n$ is very large.  The other side of the coin is that with few predictors, a simple model will likely do well, and as there are few predictors, besides the fact that the solution will be parsimonious, you can estimate the few parameters using a parametric apprach (or a more inflexible approach).*

(b) The number of predictors $p$ is extremely large, and the number
of observations $n$ is small.

*Again, as in question a) it depends on what your aim is, however with a small $n$, the approach is generally to implement a parametric approach (however it should be noted that a lot of work is being done in the field, especially with Generative Adverserial Networks to break the curse of small datasets in machine learning)*

(c) The relationship between the predictors and response is highly
non-linear.

*With non-linearity it is difficult to estimate f, however it can be done.  Generally non-parametric fits multi modal distributions better, so flexible method would be advised*

(d) The variance of the error terms, i.e. $\sigma^2$ = Var($\epsilon$), is extremely
high.
*Again, depends on what your aim is.  If the aim is to understand some form of a relationship between the target and the inputs, even if that relationship is weak, parametric will be the better approach.  With the error terms extremely high, one would imagine that that was the approach followed initially.  However, if the aim is to reduce the error terms, non-parametric or a more flexible approach would be the order of the day*

## Question 2

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction.  Finally, provide n and p.
(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

*Firstly, with the aim of understanding which factors influence the CEO salary (assuming the CEO salary is a continuous variable) the problem is one of regression.  By aiming to predict the salary, one will estimate wieghts for each of the input variables and thus gain understanding of which factor affects the CEO salary.  We have a dataset of $n = 500$ and we have $p=3$ input variables and the response of CEO salary.  If we one-hot encode the industry (as it is a categorical variable) we will have the number of industries plus the two remaining input variables as total number of inputs.*

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

*Again, we aim to predict if the product will be successful or not (so binary classification).  We have $n=20$ similar products as our sample and we collect data on $p=13$ input variables and the response of success or failure.*

(c) We are interest in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

*We aim to predict the % change in the USD/Euro exchange rate, so the assumption is that this % is continuous and therefore it is a regression problem.  For the year 2012, there was 52 weeks and as we collected data on a weekly basis we will have $n=52$ observations in our sample.  Besides our response variable of % change in the USD/Euro exchange rate, we also record $p=3$ other input variables.*

## Question 4
You will now think of some real-life applications for statistical learning. 
(a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

*Sentiment analysis can also be seen as classification.  Specifically applied in the context of social media, you will aim to classify if a tweet is positive, neutral or negative.  The response will be the label assigned to the tweet and the predictors will be the actual text of the tweet (most likely scrubbed to remove stop words etc. etc.).*

(b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

*Predicting how much to accelerate to maintain speed on current road with self-driving cars.  Response will be acccelaration applied to maintain speed, with predictors consisting of class of road (dirt, gravel, tar), current weather conditions (raining, wind speed and direction), input from front sensors, image from streaming camera etc.*

(c) Describe three real-life applications in which cluster analysis might be useful.

*In genetic studies, clustering is applied to elucidate the patterns hidden in gene expression data.  The input will be the gene expression data, consisting of millions of measurements.  Here the aim is also to understand the hidden patterns rather than predicting something specifically.*

## Question 5

What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

## Question 6

Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

## Question 9

This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.

```{r}
library(ISLR)

data("Auto")
```

(a) Which of the predictors are quantitative, and which are qualitative?
```{r message=FALSE}
library(funModeling)

df_status(Auto)
head(Auto)
```

This shows us that mpg, cylinders, displacement, horsepower, weight, acceleration, year and origin are numeric and name is qualitative. It also shows us that there are no NA or infinite values in the dataset.

(b) What is the range of each quantitative predictor? You can answer this using the range() function. range()
```{r message=FALSE}
library(dplyr)
auto_data = select(Auto, -'name')

lapply(auto_data, range)
```

(c) What is the mean and standard deviation of each quantitative predictor?
Mean
```{r}
lapply(auto_data, mean)
```

Standard deviation
```{r}
lapply(lapply(auto_data, var),sqrt)
```
(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}
reduced_set = auto_data[c(1:9,86:(dim(auto_data)[1]-1)),]
```
Mean
```{r}
lapply(reduced_set, mean)
```

Standard deviation
```{r}
lapply(lapply(reduced_set, var),sqrt)
df_status(Auto)
```

(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
```{r message=FALSE}
library(ggplot2)
library(ggbiplot)

ggplot(Auto) + geom_boxplot(aes(as.factor(Auto$cylinders), Auto$mpg)) + ggplot2::xlab('Cylinders') + ggplot2::ylab('MPG')+
    facet_wrap(facets = vars(Auto$year)) + ggtitle('Faceted by year')

ggplot(Auto) + geom_boxplot(aes(as.factor(Auto$cylinders), Auto$mpg)) + ggplot2::xlab('Cylinders') + ggplot2::ylab('MPG')+
    facet_wrap(facets = vars(Auto$origin)) + ggtitle('Faceted by origin')

ggplot(Auto) + geom_boxplot(aes(as.factor(Auto$origin), Auto$mpg)) + ggplot2::xlab('origin') + ggplot2::ylab('MPG')+
    facet_wrap(facets = vars(Auto$cylinders)) + ggtitle('Faceted by cylinders')

qplot(Auto$mpg, Auto$displacement)

qplot(Auto$mpg, Auto$horsepower)

qplot(Auto$mpg, Auto$weight)

qplot(Auto$mpg, Auto$acceleration)

fit = princomp(select(Auto, -'name'), cor=TRUE)
ggbiplot(fit)

data = as.matrix(select(Auto, -'name'))
rownames(data) = Auto$name

dd = dist(scale(select(Auto, -'name')), method = "euclidean")
hc = hclust(dd, method = "ward.D2")
hcd = as.dendrogram(hc)
heatmap(data, Colv = NA, Rowv = hcd, scale="column")
```

(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

*Yes, from the biplot and the scatter plots there is quite a clear negative correlation between mpg and cylinders, displacement, horsepower and weight.  This is eluded to in the heatmap as well, leveraging a dendrogram to assist with order the matrix.  Looking at both the biplot and the heatmap, there seems to also be a weak positive relationship between mpg and origin, but I'd rather place my focus on the stronger negative correlations.  Logically there is some sence in the correlation between mpg, cylinders, displacement, horsepower and weight, confirmed upon visual inspection, but yet to be confirmed empirically.*

